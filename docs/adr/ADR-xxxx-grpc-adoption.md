# ADR-xxxx: Принятие gRPC для внутренних высоконагруженных вызовов

- **Статус:** Принято
- **Дата:** 2026-02-10
- **Авторы:** Platform/Backend
- **Зона действия:** Внутрисервисные синхронные вызовы (service-to-service)

## Контекст

Сейчас во внутреннем контуре используются REST/JSON и частично GraphQL для межсервисных вызовов. При росте нагрузки обнаружены системные ограничения:

1. **Сериализация JSON + HTTP/1.1** создает заметный CPU overhead на горячих путях.
2. **Множественные round-trip и over-fetching** в GraphQL в read-heavy сценариях увеличивают p95 latency.
3. **Отсутствие единой строго типизированной схемы** между командами приводит к дрейфу контрактов и неявным breaking changes.
4. **Сложный streaming** (SSE/WebSocket fallback) увеличивает операционную сложность и нестабильность в пиках.

## Проблема / bottleneck’и REST/GraphQL

Для внутренних high-load сценариев зафиксированы следующие bottleneck’и:

- Рост p95/p99 из-за JSON парсинга и крупного payload (REST/GraphQL).
- Ограниченная эффективность multiplexing при HTTP/1.1 в текущем стеке gateway/service.
- Нестандартизованный подход к retries/timeouts/идемпотентности между сервисами.
- Дублирование DTO и ручная синхронизация контрактов в нескольких репозиториях.

## Требования

### 1) Требования к streaming

- Нужен **server-streaming** для доставки пачек событий без переподключения клиента.
- Допустимая задержка между событиями: до 200 мс на p95.
- Backpressure должен поддерживаться на транспортном уровне.

### 2) Требования к latency и RPS

- Целевая p95 latency для критического S2S вызова: **≤ 45 мс**.
- Целевой throughput на сервисный инстанс: **≥ 5 000 RPS** при сохранении error budget.
- Error budget: **≤ 0.1% 5xx/transport errors** на 30-дневном окне.

### 3) Требования к строгой схеме и codegen

- Требуется единый IDL с версионированием и правилами backward compatibility.
- Генерация server/client stubs должна быть автоматизирована в CI.
- Контрактные изменения должны проверяться до merge (lint + breaking-change check).

## Рассмотренный пилот

### Сценарий

Выбран внутренний high-load сценарий: **лента рекомендаций** (Recommendation API -> Ranking Service).

Почему этот сценарий:
- высокий RPS в течение всего дня;
- чувствительность к latency (влияет на TTFB ленты);
- есть потребность в server-streaming для инкрементальной выдачи кандидатов.

### Дизайн пилота

Сравнение двух реализаций в одинаковых условиях:

- **Вариант A:** REST/JSON (текущий путь).
- **Вариант B:** gRPC/Protobuf (новый путь).

Условия теста:
- одинаковая бизнес-логика и источники данных;
- одинаковые лимиты CPU/memory;
- нагрузочный профиль: 15 минут прогрева + 45 минут измерений;
- 3 повторения, метрика = медиана прогонов.

## Результаты пилота

| Метрика | REST/GraphQL baseline | gRPC pilot | Δ |
|---|---:|---:|---:|
| p95 latency | 78 мс | 41 мс | **-47%** |
| Throughput (RPS/instance) | 3 400 | 5 300 | **+56%** |
| Error rate (5xx/transport) | 0.23% | 0.08% | **-65%** |
| CPU на 1k req | 1.00x | 0.71x | **-29%** |

### Error budget

- Baseline (REST/GraphQL): 0.23% > 0.1% (budget не выполняется).
- Pilot (gRPC): 0.08% <= 0.1% (budget выполняется).

### Сложность эксплуатации (качественная оценка)

- **Плюсы gRPC:**
  - единый контракт через protobuf;
  - встроенный streaming и deadline propagation;
  - предсказуемый codegen для клиентов.
- **Минусы gRPC:**
  - усложнение observability (нужно унифицировать grpc-status/tracing);
  - требования к ingress/proxy (HTTP/2 end-to-end);
  - дополнительные правила versioning protobuf.

Итог по сложности: умеренный рост операционной сложности, но ниже, чем текущая стоимость поддержки REST+GraphQL+SSE в high-load path.

## Решение

По итогам метрик выигрыш **значимый** и покрывает стоимость внедрения.

**Решение:**
1. Внедрять gRPC **только во внутреннем контуре** для high-load S2S сценариев.
2. Внешний API (public edge) оставить на REST/GraphQL.
3. Миграцию выполнять поэтапно, начиная с Recommendation -> Ranking.

## Критерии применения решения в других сервисах

Сервис переводится на gRPC, если одновременно выполняются условия:

- p95 > 60 мс на текущем REST/GraphQL пути при нагрузке production-профиля;
- требуется streaming или строгая типизация контрактов между 2+ командами;
- ожидаемый выигрыш по p95 >= 25% и/или throughput >= 30%.

## План внедрения

1. Зафиксировать protobuf контракт и правила эволюции полей.
2. Подключить codegen в CI/CD + lint/breaking checks.
3. Включить dual-run (REST и gRPC) и сравнение SLI в canary.
4. Переключить трафик поэтапно (10% -> 50% -> 100%).
5. Оставить REST fallback на ограниченный переходный период.

## Последствия

### Положительные

- Снижение latency и рост пропускной способности на критическом пути.
- Улучшение соблюдения error budget.
- Более строгий lifecycle контрактов.

### Негативные/риски

- Повышение порога входа для команд (protobuf/gRPC tooling).
- Необходимость дисциплины по версионированию схем.
- Возможные проблемы совместимости в прокси/балансировщиках без корректной HTTP/2 конфигурации.

## Когда пересматривать решение

Вернуться к ADR, если:
- прирост p95 в новых пилотах < 15%;
- эксплуатационная стоимость gRPC выше ожидаемой (инциденты/онколл);
- требования продукта смещаются в сторону публичного API-first сценария.
